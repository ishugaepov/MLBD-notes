\section{Google File System (GFS) + Hadoop (HDFS)}
    \subsection{Требования к системе}

    \begin{enumerate}
        \item Производительность
        \item Масштабируемость (легко добавлять новые машины)
        \item Надежность
        \item Доступность
    \end{enumerate}
    \quad Мотивация к требованиям: распределенная файловая система (РФС) состоит из какого-то количества машин, которые постоянно ломаются. И поломки становятся нормой жизни, а не какими-то редкими ситуациями. Поэтому важна надежность. Еще выяснилось, что гораздо легче поддерживать большие файлы, чем много мелких, а при работе с большими данными большинство операций --- запись в конец или потоковое чтение. И возникает проблема, что несколько клиентов могут работать с одним файлом. Отсюда вытекают доступность, масштабируемость и производительность.
    
    \subsection{Система}
    \begin{itemize}
        \item \textit{GFS-client} --- много клиентов
        \item \textit{GFS-master} --- одна машина
        \item \textit{Chunk-серверы} --- на них хранятся данные, их много
    \end{itemize}
    \quad Файлы большие, хранятся следующим образом. Файл разбивается на куски (chunks) фиксированного размера (64MB). Мастер дает каждому куску глобальный идентификатор. Затем эти куски реплицируются и раскидываются по chunk-серверам, которые выбирает мастер
    \\~\\
    \\~\\
    \textbf{\textit{Master}}:
    \begin{enumerate}
        \item Хранит метаданные: что-то в духе индекса, для файла знает идентификаторы chunk'a, а для chunk'a знает, к какому файлу он принадлежит
        \item Создает / реплицирует chunk'и (для надежности, по умолчанию минимум 2 копии каждого файла)
        \item Сборка мусора: файл удален, надо освободить место, почистить метаданные
        \item HeartBeat сообщения для общения с chunk-серверами: проверка того, жив ли сервер
        \item Обрабатывает запросы связанные с метаданными, сами данные через него не проходят, иначе bottleneck
        \item Логирует все операции (для надежности)
    \end{enumerate}
    \textbf{\textit{Chunk-сервер}}:
    \begin{enumerate}
        \item Хранит данные на локальном диске
        \item Обменивается данными с пользователями и другими серверами
        \item Обменивается запросами и метаданными с мастером
    \end{enumerate}
    
    \subsection{Как это все работает, и кто что делает}
    
    \quad Несколько chunk серверов могут быть в одной подсети, и при ее отказе откажут все машины сразу. Если так получилось, что все копии какого-то файла были именно на этих машинах, то потеряем файл. Поэтому для надежности надо реплики раскидывать и между подсетями тоже, за это ответственный мастер. Что влияет на то, на каком chunk сервере будет создан кусок:
    \begin{itemize}
        \item Утилизация диска, сети
        \item Как давно создан последний кусок (наподобие кэша, если недавно создавали, то ожидаем, что его еще будут читать в ближайшее время)
        \item Местоположение сервера
    \end{itemize}
    
    \quad Когда машина падает и число реплик какого-то куска оказывается ниже заданного уровня, то происходит ре-репликация. Master через HeartBeat понял, что сервер вышел из строя, и начинает в порядке приоритета ре-реплецировать потерянные куски. Приоритет --- у кого больше копий потеряно (в простом варианте). Мастер также занимается балансировкой нагрузки, перераспределяя chunk'и между серверами. Это нужно для выравнивания нагрузки между машинами и лучшей утилизации дисков и сетей. Сборка мусора: файл сразу не удаляется, а по-особому переименовывается (вроде отметки об удаленности), и в течение заданного времени его можно восстановить. Если никто не восстановил, то, когда мастер видит удаленный файл, он освобождает его память на chunk-серверах и чистит свои метаданные. На chunk-сервере хранятся еще метаданные о том, какие chunk-id и их версии на нем лежат. Мастер может восстанавливаться по логам и делает периодические снепшоты (когда лог стал слишком длинным).

\subsection{Hadoop}
    \quad Hadoop Distributed File System (HDFS) --- open-source реализация GFS
    \begin{itemize}
        \item Master --- NameNode
        \item Chunk-сервер --- DataNode
    \end{itemize}
    \quad Глобальное отличие: В GFS клиент сам связывается с несколькими chunk-серверами, а в HDFS связывается только с одной DataNode, а она уже пересылает данные всем остальным. Также тут реализован MapReduce.

\section{MapReduce}
\subsection{Что это такое и примеры}
    \quad Лог событий рекламы в VK за день это 70GB. Писать под каждую задачу низкоуровневый код --- глупо. Заметим, что большое количество задач формулируется в виде последовательных map и reduce. А эти функции уже можно распараллелить.
    \begin{enumerate}
        \item map: $(k_1, v_1)$ $\rightarrow$ $list(k_2, v_2)$
        \item reduce: $(k_2, list(v_2))$ $\rightarrow$ $list(v2)$
    \end{enumerate}
    
    Пример \textit{WordCount}:
    \begin{enumerate}
        \item Map: слово $\rightarrow$ (word, 1)
        \item Reduce: сумма счетчиков для каждого слова
    \end{enumerate}
    
    Пример \textit{DistributedGrep}:
    \begin{enumerate}
        \item Map: ищет строчки, содержащие паттерн
        \item Reduce: ничего не делает
    \end{enumerate}
    Пример \textit{InvertedIndex}:
    \begin{enumerate}
        \item Map: документ $\rightarrow$ (word, docID)
        \item Reduce: слово $\rightarrow$ (word, list(docID)) --- чистит список идентификаторов документов
    \end{enumerate}
    
    \quad Цели MapReduce: система берет на себя все, касающееся надежности и параллелизма (отказоустойчивость, распределение данных, планирование и распределение ресурсов). Пользователь пишет две функции и отдыхает.
    
\subsection{Система}
    \quad Система состоит из мастера, MapWorker'ов, ReduceWorker'ов. Клиент указывает количество работников каждого типа. Мастер распределяет Chunk'и между работниками. MapWorker выдает разбиение (partition): берем хэш от ключа и отправляем по нему в определенный partition. Reduce должен быть инкрементальным.
    
    \quad Машины, как и раньше, могут падать. Если падает MapWorker, то перезапускается его map задача + все reduce задачи, которые зависят от него. Если падает ReduceWorker, то перезапускается только его reduce задача. При падении мастера прерывается весь процесс MapReduce вычислений.
    
    \quad Хотим минимизировать пересылку данных по сети. Мастер знает, где лежат данные, на которых хотим запускать map задачи. Идеальный вариант: запускаем map там же, где лежат данные. Если это невозможно, то мастер выбирает работника, который по топологии сети находится максимально близко к данным.
    
    \quad Мастер время от времени опрашивает работников, на которых запущены задачи, чтобы понять их статус. Задача находится в одном из состояний:
    \begin{itemize}
        \item Idle --- еще не запущена, ожидает запуска
        \item In progress
        \item Completed
    \end{itemize}
    Если Worker не отвечает в течение заданного времени на HeartBeat сообщения, то все его задачи переходят в Idle и распределяются мастером
    
    \quad Straggler --- Worker, которому надо сильно больше времени для завершения его задач в сравнении с остальными. Может быть из-за медленного железа, далеких данных и тд. В таком случае мастер запускает дублирующие задачи для задач in progress в тот момент, когда большинство задач уже completed. Задача завершится, когда завершится либо она сама, либо любой дубликат.
    
    \quad Хэш может быть плохим, но можно задать его самому, чтобы нагрузка не упала на какие-то серверы в большем масштабе.
    
    \quad В результате работы map на локальном диске работника окажется R частей данных. При этом данные на вход reduce задачи будут подавать в порядке неубывания ключей. Из-за этого можем сделать частичную сортировку за просто так: reduce = id function, тогда после завершения всех reduce задач получим файл из R частей, внутри которых данные отсортированы. Если ReduceWorker один, то просто получаем сортировку. Или же можно сделать partitioning по ключу, а не по их хэшу.
    
    \quad Можно задавать combiner функции, которые на MapWorker делают предподсчет. Например, это можно использовать в задаче WordCout, где слова естественного языка распределены по Zipf law.
    
    \quad Если в данных есть записи, которые заставляют map/reduce падать с ошибкой, то весь процесс упадет с ошибкой. Мастер определяет записи, которые несколько раз падают, и затем помечает их как bad record, которые затем можно пропустить при перезапуске задачи.
    
    \quad Counters --- map/reduce могут инкрементировать счетчики, если встречают какие-то события.
